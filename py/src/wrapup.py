from __future__ import annotations

import aiohttp
from datetime import datetime
from typing import List, Optional
from .messages import WrapupLogEntry

DEFAULT_PROMPT = r"""
You will be given a raw voice chat transcript of a D&D 5e game session. 
Be aware that the transcript is generated by an AI speech-to-text model and contains many errors, misinterpretations, and artifacts. 
You will need to internally account for and correct mistakes in order to form a coherent understanding of the story and scenes. 
After interpreting the transcript, generate a detailed and coherent recap, suitable for use as a player's reference material for subsequent sessions. 
Within each scene recap:
  - Incorporate memorable or defining quotes that occured during that scene.
  - Any items exchanged, rewards earned, discoveries, or significant plot developments should be noted in detail.
  - Highlight any funny or unexpected moments and roleplay elements.
  - For combat, it's okay to only cover the most impactful actions, but be sure to include the outcome of the encounter.
Make sure your recap follows the order of events as they unfold in the transcript. Do not make up quotes, and always attribute them.
Maintain a playful, engaging, and enthusiastic tone without being long-winded.
"""

def generate_transcript(log_entries: List[WrapupLogEntry], session_name: str) -> str:
	"""Formats session log entries into a plain text transcript file.

	Accepts a list of `WrapupLogEntry` models (wire shape from JS).
	"""
	lines = [f"# Transcript for session: {session_name}\n"]
	for entry in log_entries:
		lines.append(f"{entry.user_name}: {entry.text}")
	return "\n".join(lines)

class LLMRequestError(RuntimeError):
	"""Raised when the Gemini API returns an error.

	Attributes:
		code: error code returned by the service or HTTP status
		message: human-readable error message
	"""

	def __init__(self, code: int | str, message: str):
		self.code = code
		self.message = message
		super().__init__(f"Gemini request failed: {code} - {message}")

class GeminiWrapupGenerator:
	"""Generate a session outline using Gemini text generation REST API.

	Constructor accepts an API key (string) and a model name. Call `generate()` with the
	transcript text to produce an outline. The method is async and returns
	the generated outline string or None on error.
	"""

	def __init__(
		self,
		api_key: str,
		model: Optional[str] = None,
		prompt: Optional[str] = None,
		temperature: float = 0.2,
		max_output_tokens: int = 8192,
	):
		self.api_key = api_key
		# model should be a Gemini model id like 'gemini-2.5-flash'
		self.model = model or 'gemini-2.5-flash'
		self.endpoint = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model}:generateContent"
		# allow overriding the system prompt and generation params
		self.prompt = prompt
		self.temperature = float(temperature)
		self.max_output_tokens = int(max_output_tokens)

	async def generate(self, transcript: str, tips: Optional[List[str]] = None) -> Optional[str]:
		# Construct a system-like prompt adapted from the legacy OpenAI prompt.

		# Use the prompt passed into the constructor if provided; otherwise use default.
		system_prompt = self.prompt or DEFAULT_PROMPT
		if tips:
			tip_block = "\n".join(f"- {t}" for t in tips)
			system_prompt = system_prompt + "\nHere are some additional tips:\n" + tip_block + "\n"

		payload = {
			"system_instruction": {
				"parts": [ { "text": system_prompt } ],
			},
			"contents": {
				"parts": [ { "text": transcript } ],
			},
			"generationConfig": {
				"maxOutputTokens": self.max_output_tokens,
				"temperature": self.temperature,
				"thinkingConfig": { "thinkingBudget": -1 },
			},
		}

		headers = {
			"x-goog-api-key": self.api_key,
			"Content-Type": "application/json",
		}

		async with aiohttp.ClientSession() as session:
			async with session.post(self.endpoint, json=payload, headers=headers) as resp:
				if resp.status != 200:
					# Try to extract structured error info from the JSON body, but
					# fall back to raw text/status when unavailable.
					try:
						err_json = await resp.json()
						err_obj = err_json.get('error', {}) if isinstance(err_json, dict) else {}
						err_code = err_obj.get('code', resp.status)
						err_message = err_obj.get('message') or (await resp.text())
					except Exception:
						err_code = resp.status
						err_message = await resp.text()
					raise LLMRequestError(err_code, err_message)
				data = await resp.json()
		
		if not data:
			return None
		return data['candidates'][0]['content']['parts'][0]['text']
