import type { WrapupLogEntry } from './messages.js';
import { readLogEntriesStrict } from './logs.js';
import { toWrapupLogEntry } from './messages.js';
import fs from 'fs';
import path from 'path';
import paths from './paths.js';

export const DEFAULT_PROMPT = `
You will be given a raw voice chat transcript of a D&D 5e game session.
Be aware that the transcript is generated by an AI speech-to-text model and contains many errors, misinterpretations, and artifacts.
You will need to internally account for and correct mistakes in order to form a coherent understanding of the story and scenes.
After interpreting the transcript, generate a detailed and coherent recap, suitable for use as a player's reference material for subsequent sessions.
Within each scene recap:
  - Incorporate memorable or defining quotes that occured during that scene.
  - Any items exchanged, rewards earned, discoveries, or significant plot developments should be noted in detail.
  - Highlight any funny or unexpected moments and roleplay elements.
  - For combat, it's okay to only cover the most impactful actions, but be sure to include the outcome of the encounter.
Make sure your recap follows the order of events as they unfold in the transcript. Do not make up quotes, and always attribute them.
Maintain a playful, engaging, and enthusiastic tone without being long-winded.
`;

export function generateTranscript(logEntries: WrapupLogEntry[], sessionName: string): string {
	const lines: string[] = [`# Transcript for session: ${sessionName}\n`];
	for (const e of logEntries) {
		lines.push(`${e.user_name}: ${e.text}`);
	}
	return lines.join('\n');
}

export interface WrapupOptions extends GeminiOptions {
	sessionName: string;
	userIdMap?: Record<string, string>;
	phraseMap?: Record<string, string>;
	forceNew?: boolean; // if true, regenerate even if wrapup exists
}

/**
 * Generates a session wrap-up outline by reading logs, creating a transcript,
 * and calling the Gemini API.
 * @param opts The options for generating the wrap-up.
 * @returns The generated outline as a string.
 * @throws An error if logs are empty or the API key is missing.
 */
export async function createWrapup(opts: WrapupOptions): Promise<string> {
	// Determine session directory and wrapup path
	const sessionDir = paths.sessionDataDir(opts.sessionName);
	paths.ensureDir(sessionDir);
	const wrapupPath = path.join(sessionDir, 'wrapup.md');

	// If exists and not forcing regeneration, return it immediately (no API key required)
	if (!opts.forceNew && fs.existsSync(wrapupPath)) {
        return fs.readFileSync(wrapupPath, 'utf8');
	}

	const entries = await readLogEntriesStrict(paths.sessionLogPath(opts.sessionName));
	if (!entries.length) {
		// Let callers handle this case with user-facing messages
		throw new Error('No log entries found for session.');
	}
	const transformed = entries.map((e) => toWrapupLogEntry(e, opts.userIdMap, opts.phraseMap));
	const transcript = generateTranscript(transformed as any, opts.sessionName);

	if (!opts.apiKey) {
		throw new Error('GEMINI_API_KEY must be set to run wrapup.');
	}

	const outline = await generateOutlineWithGemini(transcript, {
		apiKey: opts.apiKey,
		model: opts.model,
		prompt: opts.prompt,
		temperature: opts.temperature,
		maxOutputTokens: opts.maxOutputTokens,
		tips: opts.tips,
	});

	const finalText = outline ?? '';
    fs.writeFileSync(wrapupPath, finalText, 'utf8');
	return finalText;
}

export interface GeminiOptions {
	apiKey: string;
	model?: string; // e.g., 'gemini-2.5-flash'
	prompt?: string;
	tips?: string[];
	temperature?: number;
	maxOutputTokens?: number;
}

export class LLMRequestError extends Error {
	code: number | string;
	constructor(code: number | string, message: string) {
		super(`Gemini request failed: ${code} - ${message}`);
		this.code = code;
		this.name = 'LLMRequestError';
	}
}

export async function generateOutlineWithGemini(
	transcript: string,
	opts: GeminiOptions,
): Promise<string> {
	const model = opts.model || 'gemini-2.5-flash';
	const endpoint = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent`;

	// Build system prompt and optionally append tips as a bulleted list
	let systemPrompt = opts.prompt || DEFAULT_PROMPT;
	if (opts.tips && opts.tips.length) {
		const tipBlock = opts.tips.map((t) => `- ${t}`).join('\n');
		systemPrompt = `${systemPrompt}\nHere are some additional tips:\n${tipBlock}\n`;
	}

	const payload = {
		system_instruction: { parts: [{ text: systemPrompt }] },
		contents: { parts: [{ text: transcript }] },
		generationConfig: {
			maxOutputTokens: opts.maxOutputTokens ?? 8192,
			temperature: opts.temperature ?? 0.2,
			thinkingConfig: { thinkingBudget: -1 },
		},
	} as any;

	const resp = await fetch(endpoint, {
		method: 'POST',
		headers: {
			'x-goog-api-key': opts.apiKey,
			'Content-Type': 'application/json',
		},
		body: JSON.stringify(payload),
	});

	if (!resp.ok) {
		let errMsg = await resp.text();
		try {
			const j = await resp.clone().json();
			if (j && typeof j === 'object' && j.error) {
				errMsg = j.error.message || errMsg;
				throw new LLMRequestError(j.error.code ?? resp.status, errMsg);
			}
		} catch (_) {
			// ignore JSON parse errors; fall through to generic error
		}
		throw new LLMRequestError(resp.status, errMsg);
	}

	const data = await resp.json();
	// Minimal extraction; structure mirrors Python implementation
	const text = data?.candidates?.[0]?.content?.parts?.[0]?.text;
	if (!text || typeof text !== 'string') {
		throw new Error('Unexpected Gemini response shape (no text found)');
	}
	return text;
}
